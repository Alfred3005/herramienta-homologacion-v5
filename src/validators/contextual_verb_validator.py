# -*- coding: utf-8 -*-
"""
CONTEXTUAL VERB VALIDATOR - Validaci√≥n Inteligente de Verbos con LLM
Sistema de validaci√≥n con dos modos: H√çBRIDO (r√°pido) y COMPLETO (detallado)

VERSI√ìN 5.41 - Sistema Jer√°rquico de Herencia Normativa
Mejoras implementadas:
- Identificaci√≥n inteligente de instituciones (sin hardcoding)
- Sistema de 4 niveles de alineaci√≥n jer√°rquica con scores granulares
- An√°lisis funci√≥n por funci√≥n con distancia jer√°rquica
- Diferenciaci√≥n entre herencia directa, del jefe directo, y lejana

NIVELES DE ALINEACI√ìN:
- NIVEL 1: Alineaci√≥n Directa (Score: 0.9) - Mismo grupo jer√°rquico
- NIVEL 2: Herencia del Jefe Directo (Score: 0.75) - Un nivel arriba
- NIVEL 3: Herencia Lejana en Organismo (Score: 0.55) - Varios niveles arriba
- NIVEL 4: No Alineado (Score: 0.0) - Fuera del marco del organismo

MODO H√çBRIDO:
- Validaci√≥n de umbrales (cantidad de verbos d√©biles)
- 1 llamada LLM global que valida TODO el puesto vs normativa
- An√°lisis granular funci√≥n por funci√≥n
- R√°pido y econ√≥mico

MODO COMPLETO:
- Validaci√≥n funci√≥n por funci√≥n con contexto normativo
- Validaci√≥n de herencia jer√°rquica detallada
- Validaci√≥n de apropiaci√≥n de alcance por nivel
- M√°xima precisi√≥n

Fecha: 2025-11-19
Versi√≥n: 5.41 - Sistema Jer√°rquico de Herencia Normativa
"""

import json
from typing import Dict, List, Optional, Tuple, Any
from dataclasses import dataclass, field
from datetime import datetime

from src.validators.shared_utilities import (
    APFContext, robust_openai_call, VERB_HIERARCHY, WEAK_VERBS,
    LOGGING_CONFIG
)

# ==========================================
# CONFIGURACI√ìN
# ==========================================

VALIDATION_CONFIG = {
    # Modo de validaci√≥n: "HYBRID" o "COMPLETE"
    "validation_mode": "HYBRID",  # Cambiar a "COMPLETE" para modo detallado

    # Umbrales para modo h√≠brido
    "weak_verb_threshold": 20,  # M√°ximo de verbos d√©biles permitidos
    "completeness_min_threshold": 0.5,  # 50% m√≠nimo de funciones esperadas
    "completeness_max_threshold": 2.0,  # 200% m√°ximo (sobrecarga)

    # Configuraci√≥n LLM
    "llm_temperature": 0.0,  # Determin√≠stico
    "llm_max_tokens": 2000,  # Aumentado para respuesta m√°s detallada
    "use_cache": True
}

# ==========================================
# NIVELES DE ALINEACI√ìN JER√ÅRQUICA (v5.41)
# ==========================================

ALIGNMENT_LEVELS = {
    "NIVEL_1_ALINEACION_DIRECTA": {
        "score": 0.9,
        "descripcion": "Atribuci√≥n encontrada EXPL√çCITAMENTE en la secci√≥n del MISMO grupo jer√°rquico",
        "distancia_jerarquica": 0,
        "tipo_herencia": "DIRECTA",
        "clasificacion": "STRONGLY_ALIGNED"
    },
    "NIVEL_2_HERENCIA_JEFE_DIRECTO": {
        "score": 0.75,
        "descripcion": "Atribuci√≥n derivable del JEFE INMEDIATO SUPERIOR (un nivel arriba)",
        "distancia_jerarquica": 1,
        "tipo_herencia": "DIRECTA",
        "clasificacion": "ALIGNED"
    },
    "NIVEL_3_HERENCIA_LEJANA": {
        "score": 0.55,
        "descripcion": "Atribuci√≥n dentro del MARCO general del organismo (varios niveles arriba)",
        "distancia_jerarquica": 2,  # 2 o m√°s
        "tipo_herencia": "LEJANA",
        "clasificacion": "PARTIALLY_ALIGNED"
    },
    "NIVEL_4_NO_ALINEADO": {
        "score": 0.0,
        "descripcion": "Atribuci√≥n fuera del marco del organismo",
        "distancia_jerarquica": None,
        "tipo_herencia": "NONE",
        "clasificacion": "NOT_ALIGNED"
    }
}

# Jerarqu√≠a de alcance de verbos (de menor a mayor)
ALCANCE_VERBOS = {
    1: {"nivel": "Operativo", "verbos": ["recopilar", "registrar", "archivar", "transcribir"]},
    2: {"nivel": "T√©cnico", "verbos": ["elaborar", "preparar", "calcular", "procesar"]},
    3: {"nivel": "Supervisi√≥n", "verbos": ["verificar", "revisar", "controlar", "validar"]},
    4: {"nivel": "Anal√≠tico", "verbos": ["evaluar", "analizar", "diagnosticar", "investigar"]},
    5: {"nivel": "Asesor√≠a", "verbos": ["proponer", "recomendar", "sugerir", "asesorar"]},
    6: {"nivel": "Coordinaci√≥n", "verbos": ["coordinar", "gestionar", "organizar", "programar"]},
    7: {"nivel": "Gerencial", "verbos": ["administrar", "supervisar", "dirigir equipos", "implementar"]},
    8: {"nivel": "Decisi√≥n", "verbos": ["aprobar", "autorizar", "resolver", "decidir"]},
    9: {"nivel": "Estrat√©gico", "verbos": ["dirigir", "establecer", "definir pol√≠ticas", "planear"]},
    10: {"nivel": "Normativo", "verbos": ["sancionar", "expedir", "decretar", "legislar"]}
}

# Mapeo de niveles jer√°rquicos a alcances apropiados
NIVEL_TO_ALCANCE = {
    "G11": (7, 10),  # Secretario: Gerencial-Normativo
    "J31": (5, 8),   # Titular OAD: Asesor√≠a-Decisi√≥n
    "M1": (4, 7),    # Director: Anal√≠tico-Gerencial
    "P3": (1, 5),    # Jefe Depto: Operativo-Asesor√≠a
}

# ==========================================
# CLASES DE DATOS
# ==========================================

@dataclass
class GlobalValidationResult:
    """Resultado de validaci√≥n global (modo h√≠brido)

    VERSI√ìN 5.41: Incluye an√°lisis detallado funci√≥n por funci√≥n con 4 niveles de herencia
    """
    is_aligned: bool  # ¬øEst√° alineado con normativa?
    alignment_level: str  # "ALIGNED" | "PARTIALLY_ALIGNED" | "NOT_ALIGNED" | "STRONGLY_ALIGNED"
    confidence: float  # 0.0 - 1.0
    reasoning: str  # Justificaci√≥n del LLM

    # An√°lisis de umbrales
    weak_verbs_count: int
    weak_verbs_threshold_passed: bool
    completeness_rate: Optional[float] = None
    completeness_threshold_passed: bool = True

    # Detecci√≥n de problemas estructurales
    structural_issues: List[str] = field(default_factory=list)
    normativa_mismatches: List[str] = field(default_factory=list)

    # Validaci√≥n de referencias institucionales (campos v4)
    institutional_references_match: bool = True  # ¬øReferencias institucionales coinciden?
    has_hierarchical_backing: bool = False  # ¬øHay herencia jer√°rquica v√°lida?

    # An√°lisis detallado (v5.41) - Incluye an√°lisis funci√≥n por funci√≥n
    detailed_analysis: Optional[Dict[str, Any]] = None  # LLM result completo con analisis_funciones

    # Metadatos
    validation_timestamp: str = field(default_factory=lambda: datetime.now().isoformat())

@dataclass
class DetailedValidationResult:
    """Resultado de validaci√≥n detallada (modo completo)"""
    # Campos requeridos (sin valor por defecto)
    function_number: int
    function_text: str
    verb: str
    normative_backed: bool
    backing_type: str  # "DIRECT" | "DERIVED" | "HIERARCHICAL" | "NOT_BACKED"
    appropriate_for_level: bool

    # Campos opcionales (con valor por defecto)
    normative_source: Optional[str] = None
    expected_alcance_range: Tuple[int, int] = (0, 0)
    actual_alcance: int = 0
    severity: str = "MINOR"  # "MINOR" | "MODERATE" | "CRITICAL"
    suggested_alternative: Optional[str] = None
    reasoning: str = ""
    confidence: float = 0.0

# ==========================================
# CLASE PRINCIPAL
# ==========================================

class ContextualVerbValidator:
    """
    Validador contextual de verbos con dos modos de operaci√≥n
    """

    def __init__(self, normativa_loader=None, context: APFContext = None):
        self.normativa_loader = normativa_loader
        self.context = context or APFContext()
        self.validation_mode = VALIDATION_CONFIG["validation_mode"]

    def set_validation_mode(self, mode: str):
        """Cambia el modo de validaci√≥n"""
        if mode not in ["HYBRID", "COMPLETE"]:
            raise ValueError(f"Modo inv√°lido: {mode}. Usar 'HYBRID' o 'COMPLETE'")
        self.validation_mode = mode
        VALIDATION_CONFIG["validation_mode"] = mode

    # ==========================================
    # MODO H√çBRIDO (R√°pido + LLM Global)
    # ==========================================

    def validate_global(
        self,
        puesto_nombre: str,
        objetivo_general: str,
        funciones: List[Dict[str, Any]],
        nivel_jerarquico: str,
        weak_verbs_detected: List[str]
    ) -> GlobalValidationResult:
        """
        Validaci√≥n global del puesto completo (modo h√≠brido)

        Pasos:
        1. Validar umbrales (verbos d√©biles, completitud)
        2. Llamar LLM para validaci√≥n sem√°ntica global
        3. Retornar resultado consolidado
        """

        # PASO 1: Validaci√≥n de umbrales
        weak_verbs_count = len(weak_verbs_detected)
        weak_threshold = VALIDATION_CONFIG["weak_verb_threshold"]
        weak_verbs_pass = weak_verbs_count <= weak_threshold

        # Validar completitud de funciones
        completeness_rate = None
        completeness_pass = True

        if self.normativa_loader:
            expected_functions = self._estimate_expected_functions_count()
            if expected_functions > 0:
                completeness_rate = len(funciones) / expected_functions
                min_thresh = VALIDATION_CONFIG["completeness_min_threshold"]
                max_thresh = VALIDATION_CONFIG["completeness_max_threshold"]
                completeness_pass = min_thresh <= completeness_rate <= max_thresh

        # Si falla umbrales, retornar FAIL inmediato
        structural_issues = []
        if not weak_verbs_pass:
            structural_issues.append(
                f"Exceso de verbos d√©biles: {weak_verbs_count} > {weak_threshold}"
            )
        if not completeness_pass and completeness_rate is not None:
            if completeness_rate < VALIDATION_CONFIG["completeness_min_threshold"]:
                structural_issues.append(
                    f"Funciones insuficientes: {completeness_rate:.1%} < 50%"
                )
            else:
                structural_issues.append(
                    f"Sobrecarga de funciones: {completeness_rate:.1%} > 200%"
                )

        # PASO 2: Validaci√≥n LLM Global
        llm_result = self._validate_with_llm_global(
            puesto_nombre=puesto_nombre,
            objetivo_general=objetivo_general,
            funciones=funciones,
            nivel_jerarquico=nivel_jerarquico
        )

        # PASO 3: Consolidar resultado con validaci√≥n estricta (v5.41)

        # Extraer datos del nuevo formato con resumen_alineacion
        resumen = llm_result.get("resumen_alineacion", {})
        institutional_match = llm_result.get("instituciones_coinciden", True)
        has_hierarchical_backing = llm_result.get("has_hierarchical_backing", False)

        # Usar clasificacion_global del resumen (calculado por LLM basado en scores de funciones)
        clasificacion_global = resumen.get("clasificacion_global", llm_result.get("alignment_level", "NOT_ALIGNED"))
        score_promedio = resumen.get("score_promedio", 0.0)
        confianza = resumen.get("confianza", llm_result.get("confidence", 0.5))

        # Si falla umbrales O instituciones no coinciden O clasificacion es NOT_ALIGNED ‚Üí NOT_ALIGNED
        if structural_issues or not institutional_match or clasificacion_global == "NOT_ALIGNED":
            alignment_level = "NOT_ALIGNED"
            is_aligned = False

            # A√±adir contexto si fue por falta de coincidencia institucional
            if not institutional_match and "reasoning" in llm_result:
                llm_result["reasoning"] += " [RECHAZADO: Organismos no coinciden]"

        # PARTIALLY_ALIGNED: Aprobar con observaciones
        elif clasificacion_global == "PARTIALLY_ALIGNED":
            alignment_level = "PARTIALLY_ALIGNED"
            is_aligned = True  # Pasa con observaciones (score_promedio >= 0.55)

        # ALIGNED o STRONGLY_ALIGNED
        else:
            alignment_level = clasificacion_global  # Puede ser "ALIGNED" o "STRONGLY_ALIGNED"
            is_aligned = True

        return GlobalValidationResult(
            is_aligned=is_aligned,
            alignment_level=alignment_level,
            confidence=confianza,
            reasoning=llm_result.get("reasoning", "Sin razonamiento disponible"),
            weak_verbs_count=weak_verbs_count,
            weak_verbs_threshold_passed=weak_verbs_pass,
            completeness_rate=completeness_rate,
            completeness_threshold_passed=completeness_pass,
            structural_issues=structural_issues,
            normativa_mismatches=llm_result.get("normativa_mismatches", []),
            institutional_references_match=institutional_match,
            has_hierarchical_backing=has_hierarchical_backing,
            detailed_analysis=llm_result  # v5.41: Incluir an√°lisis completo con analisis_funciones
        )

    def _validate_with_llm_global(
        self,
        puesto_nombre: str,
        objetivo_general: str,
        funciones: List[Dict[str, Any]],
        nivel_jerarquico: str
    ) -> Dict[str, Any]:
        """
        Llamada LLM para validaci√≥n sem√°ntica global del puesto
        """

        # Construir texto de funciones
        funciones_text = "\n".join([
            f"{i}. {f.get('verbo_accion', '')} - {f.get('descripcion_completa', '')[:150]}"
            for i, f in enumerate(funciones, 1)
        ])

        # Crear query de b√∫squeda con objetivo general + funciones principales
        search_query = f"{objetivo_general} {' '.join([f.get('descripcion_completa', '')[:100] for f in funciones[:5]])}"

        # Obtener contexto normativo relevante con b√∫squeda sem√°ntica
        normativa_context = self._get_normativa_context_summary(search_query=search_query)

        prompt = f"""Eres experto en an√°lisis de puestos APF. Eval√∫a cada funci√≥n usando sistema de 4 niveles de alineaci√≥n jer√°rquica (v5.41).

PUESTO: {puesto_nombre} | NIVEL: {nivel_jerarquico}
OBJETIVO: {objetivo_general}

FUNCIONES ({len(funciones)} total):
{funciones_text}

NORMATIVA:
{normativa_context}

INSTRUCCIONES:

1. IDENTIFICAR ORGANISMOS (inteligente, sin hardcoding):
   ‚Ä¢ Organismo del puesto: Analiza NOMBRE, OBJETIVO, FUNCIONES ‚Üí extrae nombre completo y siglas
   ‚Ä¢ Organismo normativa: Analiza t√≠tulos y art√≠culos iniciales ‚Üí extrae nombre y siglas
   ‚Ä¢ Si NO coinciden ‚Üí clasificaci√≥n autom√°tica: NOT_ALIGNED

2. IDENTIFICAR GRUPO JER√ÅRQUICO del puesto en normativa:
   Ejemplos: "Secretario", "Subsecretario", "Director General", "Director de √Årea", "Subdirector", "Jefe Depto", "Enlace"

3. CLASIFICAR CADA FUNCI√ìN en 4 NIVELES:

NIVEL 1 - ALINEACI√ìN DIRECTA (score: 0.9, distancia: 0)
‚Ä¢ Funci√≥n est√° EXPL√çCITAMENTE en secci√≥n del MISMO grupo jer√°rquico
‚Ä¢ Ejemplo: Subdirector ‚Üí Art. 25 "Los Subdirectores de √Årea tendr√°n..."
‚Ä¢ JSON: nivel="NIVEL_1_ALINEACION_DIRECTA", clasificacion="STRONGLY_ALIGNED", tipo_herencia="DIRECTA"

NIVEL 2 - HERENCIA JEFE DIRECTO (score: 0.75, distancia: 1)
‚Ä¢ Funci√≥n DERIVABLE del jefe inmediato superior (1 nivel arriba)
‚Ä¢ Ejemplo: Funci√≥n Subdirector "elaborar programas" apoya atribuci√≥n Director "coordinar"
‚Ä¢ Validar: funci√≥n subordinada m√°s operativa/t√©cnica vs jefe estrat√©gica/directiva
‚Ä¢ JSON: nivel="NIVEL_2_HERENCIA_JEFE_DIRECTO", clasificacion="ALIGNED", tipo_herencia="DIRECTA"

NIVEL 3 - HERENCIA LEJANA (score: 0.55, distancia: 2+)
‚Ä¢ Funci√≥n en marco general del organismo sin cadena directa (2+ niveles distancia)
‚Ä¢ Ejemplo: Subdirector "proponer indicadores" relacionado con Secretario "establecer pol√≠ticas" (3 niveles)
‚Ä¢ JSON: nivel="NIVEL_3_HERENCIA_LEJANA", clasificacion="PARTIALLY_ALIGNED", tipo_herencia="LEJANA"

NIVEL 4 - NO ALINEADO (score: 0.0)
‚Ä¢ Funci√≥n FUERA del marco del organismo o contradice atribuciones
‚Ä¢ JSON: nivel="NIVEL_4_NO_ALINEADO", clasificacion="NOT_ALIGNED", tipo_herencia="NONE", distancia_jerarquica=null

FORMATO JSON RESPUESTA:
{{
  "organismo_puesto": {{"nombre": "...", "siglas": "...", "confianza": 0.0-1.0, "fuente_identificacion": "..."}},
  "organismo_normativa": {{"nombre": "...", "siglas": "...", "confianza": 0.0-1.0, "fuente_identificacion": "..."}},
  "instituciones_coinciden": true/false,
  "grupo_jerarquico_puesto": {{"nivel": "...", "tipo": "Mando Medio/Alto/Operativo", "confianza": 0.0-1.0}},
  "analisis_funciones": [
    {{
      "funcion_id": "F001",
      "descripcion": "...",
      "alineacion": {{
        "nivel": "NIVEL_X",
        "clasificacion": "STRONGLY_ALIGNED|ALIGNED|PARTIALLY_ALIGNED|NOT_ALIGNED",
        "score": 0.9|0.75|0.55|0.0,
        "evidencia_normativa": {{
          "grupo_encontrado": "...",
          "articulo": "Art. X, frac. Y",
          "texto": "... (max 200 chars)",
          "distancia_jerarquica": 0-5,
          "tipo_herencia": "DIRECTA|LEJANA|NONE"
        }},
        "razonamiento": "... (2-3 oraciones)"
      }}
    }}
  ],
  "resumen_alineacion": {{
    "total_funciones": {len(funciones)},
    "nivel_1_directas": 0,
    "nivel_2_jefe_directo": 0,
    "nivel_3_lejanas": 0,
    "nivel_4_no_alineadas": 0,
    "score_promedio": 0.0-1.0,
    "clasificacion_global": "ALIGNED|PARTIALLY_ALIGNED|NOT_ALIGNED",
    "confianza": 0.0-1.0
  }},
  "reasoning": "... (3-4 oraciones resumen ejecutivo)",
  "institutional_references_match": true/false,
  "has_hierarchical_backing": true/false,
  "normativa_mismatches": []
}}

Eval√∫a TODAS las {len(funciones)} funciones. Responde SOLO con JSON v√°lido.
"""

        try:
            response = robust_openai_call(
                prompt=prompt,
                model="openai/gpt-4o-mini",  # Migrado a GPT-4o-mini (ahorro 94.6%)
                max_tokens=VALIDATION_CONFIG["llm_max_tokens"],
                temperature=VALIDATION_CONFIG["llm_temperature"],
                context=self.context
            )

            # Verificar respuesta exitosa
            if response.get("status") == "success":
                result = response["data"]
                return result
            else:
                # Error en llamada LLM - RECHAZAR por seguridad
                error_msg = response.get("error", "Error desconocido")
                print(f"‚ö†Ô∏è Error en validaci√≥n LLM global: {error_msg}")
                return {
                    "alignment_level": "NOT_ALIGNED",
                    "confidence": 0.3,
                    "reasoning": f"Error en validaci√≥n LLM: {error_msg}. Por seguridad se rechaza el puesto.",
                    "institutional_references_match": False,
                    "references_found_in_puesto": [],
                    "references_found_in_normativa": [],
                    "has_hierarchical_backing": False,
                    "hierarchical_reasoning": "",
                    "normativa_mismatches": ["Error t√©cnico en validaci√≥n"],
                    "strengths": [],
                    "improvement_areas": []
                }

        except Exception as e:
            print(f"‚ö†Ô∏è Error inesperado en validaci√≥n LLM global: {e}")
            # Fallback seguro - RECHAZAR por seguridad
            return {
                "alignment_level": "NOT_ALIGNED",
                "confidence": 0.3,
                "reasoning": f"Error inesperado: {str(e)}. Por seguridad se rechaza el puesto.",
                "institutional_references_match": False,
                "references_found_in_puesto": [],
                "references_found_in_normativa": [],
                "has_hierarchical_backing": False,
                "hierarchical_reasoning": "",
                "normativa_mismatches": ["Error t√©cnico en validaci√≥n"],
                "strengths": [],
                "improvement_areas": []
            }

    # ==========================================
    # MODO COMPLETO (LLM Contextual Detallado)
    # ==========================================

    def validate_detailed(
        self,
        function_text: str,
        verb: str,
        nivel_jerarquico: str,
        function_number: int
    ) -> DetailedValidationResult:
        """
        Validaci√≥n detallada de una funci√≥n individual (modo completo)

        Valida:
        1. Respaldo normativo (DIRECTO / DERIVADO / JER√ÅRQUICO / NO_RESPALDADO)
        2. Apropiaci√≥n de verbo para el nivel
        3. Alcance apropiado
        """

        # Obtener chunks relevantes de normativa para esta funci√≥n
        relevant_chunks = self._get_relevant_normativa_chunks(function_text)

        prompt = f"""Eres un experto en an√°lisis de descripciones de puestos de la APF.

FUNCI√ìN A EVALUAR:
- N√∫mero: {function_number}
- Verbo: "{verb}"
- Descripci√≥n completa: "{function_text}"
- Nivel del puesto: {nivel_jerarquico}

NORMATIVA RELEVANTE:
{relevant_chunks}

JERARQU√çA DE VERBOS POR NIVEL:
- G11 (Secretario): dirigir, establecer, autorizar, aprobar, sancionar (alcance 7-10)
- J31 (Titular OAD): coordinar, supervisar, administrar, evaluar (alcance 5-8)
- M1 (Director): administrar, gestionar, implementar, supervisar (alcance 4-7)
- P3 (Jefe Depto): ejecutar, implementar, recopilar, elaborar (alcance 1-5)

ESCALA DE ALCANCE:
1. Operativo: recopilar, registrar
2. T√©cnico: elaborar, preparar
3. Supervisi√≥n: verificar, revisar
4. Anal√≠tico: evaluar, analizar
5. Asesor√≠a: proponer, recomendar
6. Coordinaci√≥n: coordinar, gestionar
7. Gerencial: administrar, supervisar
8. Decisi√≥n: aprobar, autorizar
9. Estrat√©gico: dirigir, establecer
10. Normativo: sancionar, expedir

TAREA:
Eval√∫a si esta funci√≥n est√° RESPALDADA por la normativa considerando:

1. RESPALDO DIRECTO: ¬øLa funci√≥n est√° expl√≠citamente en la normativa?
2. RESPALDO DERIVADO: ¬øEs una derivaci√≥n l√≥gica de una funci√≥n superior?
3. RESPALDO JER√ÅRQUICO: ¬øEl verbo es apropiado para el nivel del puesto?

RESPONDE EN JSON:
{{
  "normative_backed": true/false,
  "backing_type": "DIRECT" | "DERIVED" | "HIERARCHICAL" | "NOT_BACKED",
  "normative_source": "cita textual de normativa si aplica",
  "appropriate_for_level": true/false,
  "actual_alcance": 1-10,
  "severity": "MINOR" | "MODERATE" | "CRITICAL",
  "suggested_alternative": "verbo alternativo si aplica",
  "reasoning": "justificaci√≥n clara (2-3 oraciones)",
  "confidence": 0.0-1.0
}}
"""

        try:
            response = robust_openai_call(
                prompt=prompt,
                model="openai/gpt-4o-mini",  # Migrado a GPT-4o-mini (ahorro 94.6%)
                max_tokens=VALIDATION_CONFIG["llm_max_tokens"],
                temperature=VALIDATION_CONFIG["llm_temperature"],
                context=self.context
            )

            # Verificar respuesta exitosa
            if response.get("status") != "success":
                error_msg = response.get("error", "Error desconocido")
                print(f"‚ö†Ô∏è Error en validaci√≥n LLM detallada: {error_msg}")
                # Fallback a resultado de error
                expected_range = NIVEL_TO_ALCANCE.get(nivel_jerarquico, (1, 10))
                return DetailedValidationResult(
                    function_number=function_number,
                    function_text=function_text,
                    verb=verb,
                    normative_backed=False,
                    backing_type="NOT_BACKED",
                    appropriate_for_level=False,
                    expected_alcance_range=expected_range,
                    severity="CRITICAL",
                    reasoning=f"Error en validaci√≥n: {error_msg}",
                    confidence=0.3
                )

            # Parsear respuesta exitosa
            llm_result = response["data"]

            # Determinar alcance esperado para el nivel
            expected_range = NIVEL_TO_ALCANCE.get(nivel_jerarquico, (1, 10))

            return DetailedValidationResult(
                function_number=function_number,
                function_text=function_text,
                verb=verb,
                normative_backed=llm_result.get("normative_backed", False),
                backing_type=llm_result.get("backing_type", "NOT_BACKED"),
                normative_source=llm_result.get("normative_source"),
                appropriate_for_level=llm_result.get("appropriate_for_level", False),
                expected_alcance_range=expected_range,
                actual_alcance=llm_result.get("actual_alcance", 0),
                severity=llm_result.get("severity", "CRITICAL"),
                suggested_alternative=llm_result.get("suggested_alternative"),
                reasoning=llm_result.get("reasoning", ""),
                confidence=llm_result.get("confidence", 0.5)
            )

        except Exception as e:
            print(f"‚ö†Ô∏è Error inesperado en validaci√≥n LLM detallada: {e}")
            # Fallback seguro
            return DetailedValidationResult(
                function_number=function_number,
                function_text=function_text,
                verb=verb,
                normative_backed=False,
                backing_type="NOT_BACKED",
                appropriate_for_level=False,
                severity="CRITICAL",
                reasoning=f"Error en validaci√≥n: {str(e)}",
                confidence=0.3
            )

    # ==========================================
    # M√âTODOS AUXILIARES
    # ==========================================

    def _get_normativa_context_summary(self, search_query: str = "") -> str:
        """Obtiene resumen del contexto normativo con contenido relevante

        Args:
            search_query: Query para b√∫squeda sem√°ntica de chunks relevantes

        Returns:
            Texto con metadata y chunks de normativa
        """
        if not self.normativa_loader or not hasattr(self.normativa_loader, 'documents'):
            return "No hay normativa cargada"

        summaries = []

        # Primero: metadata de documentos
        for doc_id, doc in self.normativa_loader.documents.items():
            doc_title = getattr(doc, 'title', 'Documento normativo')
            doc_type = getattr(doc, 'document_type', 'Reglamento')
            word_count = getattr(doc, 'word_count', 0)
            summaries.append(f"- {doc_title} ({doc_type}, {word_count} palabras)")

        metadata = "\n".join(summaries[:3])

        # Segundo: contenido relevante mediante b√∫squeda sem√°ntica
        content_chunks = []
        if search_query:
            # Buscar chunks relevantes usando semantic search
            try:
                search_results = self.normativa_loader.semantic_search(
                    query=search_query,
                    max_results=15,  # Traer top 15 chunks m√°s relevantes (m√°s contenido)
                    use_cache=True
                )

                for i, match in enumerate(search_results, 1):
                    # Obtener contenido completo del chunk, no solo snippet
                    doc = self.normativa_loader.documents.get(match.document_id)
                    if doc and hasattr(doc, 'semantic_chunks'):
                        chunk_idx = match.position_info.get('chunk_index', 0)
                        if chunk_idx < len(doc.semantic_chunks):
                            full_chunk = doc.semantic_chunks[chunk_idx]
                            content_chunks.append(
                                f"[Fragmento {i} - Similitud: {match.confidence_score:.2f}]\n{full_chunk[:1500]}"
                            )
            except Exception as e:
                print(f"‚ö†Ô∏è Error en b√∫squeda sem√°ntica de contexto: {e}")
                # Fallback mejorado: tomar m√∫ltiples chunks iniciales de cada documento
                for doc_id, doc in list(self.normativa_loader.documents.items())[:2]:
                    if hasattr(doc, 'semantic_chunks') and doc.semantic_chunks:
                        # Tomar los primeros 5 chunks de cada documento
                        for i, chunk in enumerate(doc.semantic_chunks[:5], 1):
                            content_chunks.append(f"[Chunk inicial {i}]\n{chunk[:1200]}")

        # Construir resumen completo
        result = f"DOCUMENTOS NORMATIVOS CARGADOS:\n{metadata}\n\n"

        if content_chunks:
            result += "CONTENIDO NORMATIVO RELEVANTE:\n"
            result += "\n\n".join(content_chunks)
        else:
            result += "No se encontraron fragmentos relevantes espec√≠ficos."

        return result

    def _get_relevant_normativa_chunks(self, function_text: str, max_chunks: int = 3) -> str:
        """Obtiene chunks de normativa relevantes para una funci√≥n"""
        if not self.normativa_loader or not hasattr(self.normativa_loader, 'documents'):
            return "No hay normativa cargada"

        # TODO: Implementar b√∫squeda sem√°ntica con embeddings
        # Por ahora, retornar primeros chunks de cada documento
        chunks_text = []
        for doc_id, doc in list(self.normativa_loader.documents.items())[:2]:
            if hasattr(doc, 'chunks') and doc.chunks:
                chunk = doc.chunks[0]
                content = getattr(chunk, 'content', '')
                chunks_text.append(content[:300] + "...")

        return "\n\n".join(chunks_text) if chunks_text else "No hay chunks disponibles"

    def _estimate_expected_functions_count(self) -> int:
        """Estima cantidad esperada de funciones basada en normativa"""
        if not self.normativa_loader or not hasattr(self.normativa_loader, 'documents'):
            return 0

        # Buscar menciones de "funci√≥n" o "atribuci√≥n" en la normativa
        # Por ahora, retornar un estimado conservador
        # TODO: Parsear funciones de la normativa de forma estructurada
        return 20  # Placeholder

    def get_validation_summary(self) -> Dict[str, Any]:
        """Obtiene resumen del validador"""
        return {
            "validation_mode": self.validation_mode,
            "weak_verb_threshold": VALIDATION_CONFIG["weak_verb_threshold"],
            "completeness_thresholds": {
                "min": VALIDATION_CONFIG["completeness_min_threshold"],
                "max": VALIDATION_CONFIG["completeness_max_threshold"]
            },
            "normativa_loaded": self.normativa_loader is not None
        }


# ==========================================
# FUNCI√ìN FACTORY
# ==========================================

def create_contextual_validator(normativa_loader=None, context: APFContext = None, mode: str = "HYBRID"):
    """Crea un validador contextual con el modo especificado"""
    validator = ContextualVerbValidator(normativa_loader, context)
    validator.set_validation_mode(mode)
    return validator


# ==========================================
# TESTING
# ==========================================

if __name__ == "__main__":
    print("üß™ CONTEXTUAL VERB VALIDATOR - Test")
    print("=" * 60)

    validator = create_contextual_validator(mode="HYBRID")
    print(f"‚úÖ Validador creado en modo: {validator.validation_mode}")
    print(json.dumps(validator.get_validation_summary(), indent=2))
